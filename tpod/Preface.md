# 序言与路线图

扩散模型已迅速成为生成建模的核心范式，相关研究遍布机器学习、计算机视觉、自然语言处理等诸多领域。这些文献分散在不同社区，从不同维度推动进展：包括关乎建模原理、训练目标、采样器设计及其背后数学思想的理论基础；涵盖工程实践与架构选择的实现进展；将模型适配到特定领域或任务的实际应用；以及提升计算、内存与部署效率的系统级优化。

本专著旨在为扩散模型提供**原则性的基础**，围绕以下中心主题展开：

- **概念与表述**：我们介绍支撑扩散模型研究的基本概念与表述，为读者提供把握更广泛文献所需的核心理解。我们并不全面综述所有变体或领域应用，而是建立一个稳定的概念基础，以便在此基础上理解后续发展。
- **三种视角**：与从噪声到数据学习直接映射的经典生成模型不同，扩散模型将生成视为随时间进行的**渐近变换**，由粗结构逐步细化出细节。这一核心思想通过三种主要视角发展而来，即**变分**、**基于分数**与**基于流**的方法，它们为理解与实现扩散建模提供了互补的途径。我们聚焦于这些表述的核心原理与基础，力图追溯其关键思想的来源、厘清不同表述之间的关系，并形成将直观洞察与严格数学表述联系起来的连贯理解。
- **延伸发展**：在此基础上，我们探讨扩散模型如何进一步发展，以更高效地生成样本、对生成过程施加更多控制，并催生以扩散原理为基础的独立生成建模形式。

**读者与目标**  
本专著面向具备深度学习基础（例如了解何为神经网络以及训练如何运作）、或更具体地说具备深度生成建模基础的研究者、研究生与实践者，并希望超越表面了解而深化对扩散模型的理解。阅读结束后，读者将对扩散建模的基础形成原则性的理解、能够在统一框架下解读不同表述，并具备既有信心地应用现有模型、又能开展新研究方向所需的背景。

---

## 本专著路线图

本专著系统介绍扩散模型的基础，并追溯至其核心底层原理。

**建议阅读顺序**  
建议按书中顺序阅读，以建立整体理解。标为**可选**的章节可被已熟悉基础的读者跳过。例如，熟悉深度生成模型（DGM）的读者可略过深层生成建模概览一章；同样，若已掌握变分自编码器（VAE）、基于能量的模型（EBM）或正态化流（Normalizing Flows）的入门内容，也可跳过相应介绍性小节。其他可选部分对进阶或专题提供更深入见解，可按需查阅。

全书分为四个主要部分。

---

### Part A 与 Part B：扩散模型的基础

本部分通过回顾塑造该领域的三种基础视角，追溯扩散模型的起源。Part B 路线图（见下文图）给出了该部分的概览。

**Part A：深度生成建模（DGM）导论**  
我们从深度生成建模的基本目标出发：从一组数据样本开始，目标是构建一个能够产生看起来来自同一底层、且通常未知的数据分布的新样本的模型。许多方法通过显式概率模型或隐式学习变换来学习数据的分布，进而实现这一目标。我们随后说明这类模型如何用神经网络表示数据分布、如何从样本中学习、以及如何生成新样本。该章以主要生成框架的分类作结，突出其核心思想与关键区别。

**图 1：扩散模型视角的时间线。** 同一组使用相同颜色。  
- 变分视角：变分自编码器（VAE）→ 扩散概率模型（DPM）→ DDPM。  
- 基于分数的视角：基于能量的模型（EBM）→ 噪声条件分数网络（NCSN）→ 分数 SDE。  
- 基于流的视角：正态化流（NF）→ 神经 ODE（NODE）→ 流匹配（FM）。

**图 2：Part B——扩散模型的统一与原则性视角。** 本图将经典生成建模方法（变分自编码器、基于能量的模型、正态化流）与其对应的扩散模型表述在视觉上联系起来。每条纵向路径表示一条概念脉络，最终汇聚到连续时间框架。三种视角（变分、基于分数、基于流）提供不同但在数学上等价的解释。

**Part B：扩散模型的核心视角**  
在概述了深度生成建模的一般目标与机制之后，我们转向扩散模型——一类将生成实现为从噪声到数据的**渐近变换**的方法。我们考察三个相互关联的框架，其共同特征是：前向过程逐步加噪，逆向时间过程由一系列执行逐步去噪的模型近似。

- **变分视角**：源于变分自编码器（VAE），将扩散表述为通过变分目标学习去噪过程，从而得到去噪扩散概率模型（DDPM）。
- **基于分数的视角**：植根于基于能量的模型（EBM），并发展为噪声条件分数网络（NCSN）。该视角学习**分数函数**（即数据对数密度的梯度），用以指导如何从样本中逐步去除噪声。在连续时间下，分数 SDE 一章引入**分数 SDE 框架**，将该去噪过程描述为随机微分方程（SDE），其确定性对应则用常微分方程（ODE）描述。这一视角将扩散建模与经典微分方程理论联系起来，为分析与算法设计提供了清晰的数学基础。
- **基于流的视角**：建立在正态化流之上，并由流匹配推广。该视角将生成建模为连续变换，将样本从简单先验输运到数据分布；演化由 ODE 下的速度场支配，显式定义概率质量如何随时间移动。基于流的表述自然地从“先验到数据”的生成推广到更一般的**分布到分布翻译**问题，即学习连接任意源分布与目标分布的流。

尽管这些视角初看不同，后续章节将表明它们深层相通：都采用**条件化策略**，将学习目标转化为可处理的回归问题；在更深层次上，它们都描述概率分布从先验到数据的同一时间演化。该演化由 **Fokker–Planck 方程**支配，可视为密度在连续时间下的变量替换公式，保证了随机表述与确定性表述的一致性。

由于扩散模型可视为将一种分布输运到另一种分布的方法，书中还发展了它们与经典最优传输及 Schrödinger 桥的联系（后者可理解为带熵正则的最优传输）。我们回顾静态与动态表述，并说明它们与连续性方程及 Fokker–Planck 视角的关系。该章对侧重实践的读者为可选，但为希望深入研读这些联系的读者提供了严格的数学背景与经典文献索引。

---

### Part C 与 Part D：控制与加速扩散采样

在统一了基础原理之后，我们转向利用扩散模型进行高效生成的实用层面。从扩散模型采样对应于求解微分方程，但该过程通常计算昂贵。Part C 与 Part D 聚焦于通过改进采样与学习型加速技术，提升生成质量、可控性与效率。

**Part C：从扩散模型采样**  
扩散模型的生成过程呈现鲜明的由粗到细的细化：噪声被逐步去除，得到结构与细节日益连贯的样本。这一性质有利有弊。有利之处在于便于细粒度控制：通过在学到的、依赖时间的速度场上加入引导项，可以驾驭 ODE 流以反映用户意图，使采样可控。不利之处在于所需的迭代积分使采样相比单步生成器更慢。本部分专注于在推断阶段改进生成过程，而无需重新训练。

- **驾驭生成**：分类器引导与无分类器引导等技术使生成过程能够以用户定义的目标或属性为条件。在此基础上，我们进一步讨论如何利用偏好数据集使扩散模型更好地与这些偏好对齐。
- **用数值求解器加速生成**：采用先进的数值求解器以更少步数近似逆向过程，可在保持质量的同时显著加速采样、降低成本。

**Part D：学习快速生成模型**  
除改进现有采样算法外，我们探讨如何直接学习逼近扩散过程的快速生成器。

- **基于蒸馏的方法**：该思路侧重训练学生模型以模仿预训练的、较慢的扩散模型（教师）的行为。目标并非缩小教师规模，而是用少得多的积分步数（往往仅几步甚至一步）复现其采样轨迹或输出分布。
- **从零学习**：由于扩散模型中的采样可视为求解 ODE，该思路直接从零学习解映射（即流映射），而不依赖教师模型。学到的映射可将噪声直接映到数据，或更一般地沿解轨迹实现任意时刻到任意时刻的跳跃。

---

### 附录

为确保所有读者都能跟上，附录为基础概念提供背景。微分方程附录提供已成为扩散模型语言的微分方程速成。扩散模型背后（尽管视角与起源多样）的核心洞察在于**变量替换公式**。这一基础自然延伸到更深的概念，如 **Fokker–Planck 方程**与**连续性方程**，它们描述概率密度在由函数（离散时间）或微分方程（连续时间）定义的映射下如何变换与演化。连续性附录提供温和的入门，将这些基础思想与更进阶的概念衔接。在 Itô 附录中，我们介绍支撑扩散模型的两个重要但常被忽视的工具：**Itô 公式**与**Girsanov 定理**，它们为 Fokker–Planck 方程与逆向时间采样过程提供严格支撑。最后，证明附录汇集正文中讨论的部分命题与定理的证明。

---

### 本专著涵盖与未涵盖的内容

我们追求**持久性**。从自上而下的视角，本专著从一条原则出发：构造连续时间动力学，将简单先验输运到数据分布，并保证每一时刻的边缘分布与由“从数据到噪声”的给定前向过程诱导的边缘一致。从这一原则出发，我们发展出实现采样的随机流与确定性流，说明如何驾驭轨迹（引导），以及如何加速它（数值求解器）。随后我们研究受扩散启发的快速生成器，包括蒸馏方法与流映射模型。借助这些工具，读者可以将新论文纳入统一模板、理解方法为何有效，并设计改进模型。

我们并不试图对扩散模型文献做穷尽式综述，也不罗列架构、训练实践、超参数，不比较各方法的实证结果，不覆盖数据集与排行榜，不描述领域或模态特定的应用，不涉及系统级部署、大规模训练配方或硬件工程。这些主题变化迅速，更适合由专题综述、开放仓库与实现指南来覆盖。

---

*源文件：`arXiv-2510.21890v1/Preface.tex`。图中引用已改为“对应章节”或章节主题说明；人名未译。*
